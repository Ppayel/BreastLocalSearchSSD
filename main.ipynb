{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ppayel/BreastLocalSearchSSD/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dDf1QodwWwq"
      },
      "source": [
        "**Get the dataset from Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDv_b2n8wIZb"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXObHJ7Mwdko"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d 'skooch/ddsm-mammography'\n",
        "!unzip -q '/content/ddsm-mammography.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP6QzELcweZZ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**data pre-processing**"
      ],
      "metadata": {
        "id": "hTP5burQ-aIy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro5TvBdRwgmx"
      },
      "outputs": [],
      "source": [
        "images=[]\n",
        "labels=[]\n",
        "feature_dictionary = {\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'label_normal': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'image': tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "Image_height = 224\n",
        "Image_width = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtLF8HJ-wOwZ"
      },
      "outputs": [],
      "source": [
        "def _parse_function(example, feature_dictionary=feature_dictionary):\n",
        "    parsed_example = tf.io.parse_example(example, feature_dictionary)\n",
        "    return parsed_example\n",
        " \n",
        "def read_data(filename):\n",
        "    full_dataset = tf.data.TFRecordDataset(filename,num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
        "    print(full_dataset)\n",
        "    full_dataset = full_dataset.shuffle(buffer_size=31000)\n",
        "    full_dataset = full_dataset.cache()\n",
        "    print(\"Size of Training Dataset: \", len(list(full_dataset)))\n",
        "    \n",
        "    feature_dictionary = {\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'label_normal': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'image': tf.io.FixedLenFeature([], tf.string)\n",
        "    }   \n",
        " \n",
        "    full_dataset = full_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    print(full_dataset)\n",
        "    for image_features in full_dataset:\n",
        "        image = image_features['image'].numpy()\n",
        "        image = tf.io.decode_raw(image_features['image'], tf.uint8)\n",
        "        image = tf.reshape(image, [299, 299])        \n",
        "        image=image.numpy()\n",
        "        image=cv2.resize(image,(Image_height,Image_width))\n",
        "        image=cv2.merge([image,image,image])        \n",
        "        #plt.imshow(image)\n",
        "        images.append(image)\n",
        "        labels.append(image_features['label_normal'].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIElih9Nwv5y"
      },
      "outputs": [],
      "source": [
        "filenames=['/content/training10_3/training10_3.tfrecords',\n",
        "#           '/content/training10_0/training10_0.tfrecords',\n",
        "#          '/content/training10_2/training10_2.tfrecords',\n",
        "#          '/content/training10_3/training10_3.tfrecords',\n",
        "#          '/content/training10_4/training10_4.tfrecords'\n",
        "          ]\n",
        " \n",
        "for file in filenames:\n",
        "    read_data(file)\n",
        " \n",
        "print(len(images))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHDSY6MVwzFJ"
      },
      "outputs": [],
      "source": [
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBV8w1wr7_Sw"
      },
      "outputs": [],
      "source": [
        "plt.imshow(images[19])\n",
        "print(images[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split dataset into training, validation and testing sets**"
      ],
      "metadata": {
        "id": "EMfcNLwGZYQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, stratify = labels, random_state=10)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.50, stratify = y_test, random_state=50)"
      ],
      "metadata": {
        "id": "TuYBa1mFhHx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fveXbhlyENR"
      },
      "source": [
        "**Attention-aided VGG16**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lno-tazmxnjJ"
      },
      "outputs": [],
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda\n",
        "from keras.models import Model\n",
        "in_lay = Input(x_train.shape[1:])\n",
        "base_pretrained_model = VGG16(input_shape = x_train.shape[1:], include_top = False, weights = 'imagenet')\n",
        "base_pretrained_model.trainable = False\n",
        "pt_features = base_pretrained_model(in_lay)\n",
        "pt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\n",
        "pt_features = base_pretrained_model(in_lay)\n",
        "from keras.layers import BatchNormalization\n",
        "bn_features = BatchNormalization()(pt_features)\n",
        "# here we do an attention mechanism to turn pixels in the GAP on an off\n",
        "attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)\n",
        "attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
        "attn_layer = LocallyConnected2D(1, \n",
        "                                kernel_size = (1,1), \n",
        "                                padding = 'valid', \n",
        "                                activation = 'sigmoid')(attn_layer)\n",
        "                        \n",
        "# fan it out to all of the channels\n",
        "up_c2_w = np.ones((1, 1, 1, pt_depth))\n",
        "up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n",
        "               activation = 'linear', use_bias = False, weights = [up_c2_w])\n",
        "up_c2.trainable = False\n",
        "attn_layer = up_c2(attn_layer)\n",
        "\n",
        "mask_features = multiply([attn_layer, bn_features])\n",
        "gap_features = GlobalAveragePooling2D()(mask_features)\n",
        "gap_mask = GlobalAveragePooling2D()(attn_layer)\n",
        "\n",
        "# to account for missing values from the attention model\n",
        "gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n",
        "gap_dr = Dropout(0.5)(gap)\n",
        "dr_steps = Dropout(0.25)(Dense(128, activation = 'elu')(gap_dr))\n",
        "out_layer = Dense(1, activation = 'sigmoid')(dr_steps)\n",
        "\n",
        "final_model = Model(inputs = [in_lay], outputs = [out_layer])\n",
        "final_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training of the model**"
      ],
      "metadata": {
        "id": "K8DxxWn6-pqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "weight_path=\"/content/{}_weights.best.hdf5\".format('breast_cancer')\n",
        "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
        "                             save_best_only=True, mode='min', save_weights_only = True)\n",
        "\n",
        "\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.00001)\n",
        "#reduceLROnPlat = ReduceLROnPlateau()\n",
        "early = EarlyStopping(monitor=\"val_loss\", \n",
        "                      mode=\"min\", \n",
        "                      patience=5) # probably needs to be more patient, but kaggle time is limited\n",
        "callbacks_list = [checkpoint, early, reduceLROnPlat]"
      ],
      "metadata": {
        "id": "PmB4dQGciNAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "final_model.compile(tf.keras.optimizers.Adam(learning_rate=.001), \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "O7-bGW9pijbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "\n",
        "history = final_model.fit(x_train, y_train, \n",
        "                          steps_per_epoch=len(x_train)//64,\n",
        "                          epochs=30,\n",
        "                          validation_data = (x_val, y_val),\n",
        "                          validation_steps = len(x_val)//64, \n",
        "                          callbacks = callbacks_list\n",
        "                        )\n",
        "\n",
        "loss_value , accuracy = final_model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Test_loss_value = ' +str(loss_value))\n",
        "print('test_accuracy = ' + str(accuracy))"
      ],
      "metadata": {
        "id": "RENljyvvR8H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOTTING RESULTS (Train vs Validation) \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def Train_Val_Plot(acc,val_acc,loss,val_loss):\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize= (18,8))\n",
        "    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n",
        "    ax1.set_xticks(np.arange(1, 21, 1))\n",
        "    \n",
        "    ax1.plot(range(1, len(acc) + 1), acc)\n",
        "    ax1.plot(range(1, len(val_acc) + 1), val_acc)\n",
        "    #ax1.set_title('History of Accuracy')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(['training', 'validation'])\n",
        "    #plt.xticks(np.arange(1, 21, 1))\n",
        "    ax2.set_xticks(np.arange(1, 21, 1))\n",
        "    ax2.plot(range(1, len(loss) + 1), loss)\n",
        "    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n",
        "    #ax2.set_title('History of Loss')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(['training', 'validation']) \n",
        "    \n",
        "    plt.show()\n",
        "    fig.savefig('figure.png')\n",
        "    \n",
        "Train_Val_Plot(history.history['acc'],history.history['val_acc'],\n",
        "               history.history['loss'],history.history['val_loss'])"
      ],
      "metadata": {
        "id": "o1hj_NRvj62q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extraction of Deep Features and Classification with KNN Classifier**"
      ],
      "metadata": {
        "id": "BWvoYLF4-2QC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFYQhgCwyePZ"
      },
      "outputs": [],
      "source": [
        "predictions = final_model.layers[-2].output\n",
        "\n",
        "model_feat = keras.Model(inputs = final_model.inputs ,outputs = predictions)\n",
        "\n",
        "Extracted_features = model_feat.predict(x_test)\n",
        "print(Extracted_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_image, valid_image, train_label, valid_label = train_test_split(Extracted_features, y_test, test_size=0.20, stratify = y_test, random_state=10)\n",
        "val_img, test_img, val_lab, test_lab = train_test_split(valid_image, valid_label, test_size=0.5, stratify = valid_labels, random_state=50)"
      ],
      "metadata": {
        "id": "puJwxCwYgHVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "_classifier = KNeighborsClassifier(n_neighbors=5)"
      ],
      "metadata": {
        "id": "YyB2-s7B3LUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_classifier.fit(train_image, train_label)"
      ],
      "metadata": {
        "id": "wvSTLz3alF3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = _classifier.predict(test_img)\n",
        "acc = accuracy_score(y_true = test_lab, y_pred = pred)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "v03CWeaGSK__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying FS with different FS algorithm on the extracted deep features**"
      ],
      "metadata": {
        "id": "2NUzWeXc_JTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_features = Extracted_features\n",
        "lab = np.array(y_test)\n",
        "lab = lab.reshape(lab.shape[0], 1)\n",
        "print(final_features.shape)\n",
        "print(lab.shape)\n",
        "np.savetxt(\"ext_features.csv\", final_features, delimiter=\",\")\n",
        "np.savetxt(\"labels.csv\", lab, delimiter=\",\")"
      ],
      "metadata": {
        "id": "AhUzWZxPwMn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/ext_features.csv', header=None)\n",
        "df2 = pd.read_csv('/content/labels.csv', header=None)\n",
        "\n",
        "print(df.shape)\n",
        "print(df2.shape)\n",
        "total_features=df.shape[1]\n",
        "x=df[df.columns[:total_features]]\n",
        "y=df2[df2.columns[-1]].astype(int)\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(total_features)"
      ],
      "metadata": {
        "id": "TkZGl-L11E0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, stratify = y, random_state=10)\n",
        "print(train_x.shape)\n",
        "print(test_y.shape)"
      ],
      "metadata": {
        "id": "TjnKQVO_onL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm: Local Search based SSD** "
      ],
      "metadata": {
        "id": "t1BA7CmGvdY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#adaptivebeta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "swarm_size = 20   #population size\n",
        "max_iterations = 100\n",
        "omega = 0.2  #used in the fitness function\n",
        "delta=0.2   #to set an upper limit for including a slightly worse particle in LAHC\n",
        "\n",
        "\n",
        "def mutate(agent):\n",
        "  percent=0.2\n",
        "  numChange=int(total_features*percent)\n",
        "  pos=np.random.randint(0,total_features-1,numChange) #choose random positions to be mutated\n",
        "  agent[pos]=1-agent[pos] #mutation\n",
        "  return agent\n",
        "\n",
        "def LAHC(particle):\n",
        "            _lambda = 15 #upper limit on number of iterations in LAHC\n",
        "            target_fitness = find_fitness(particle) #original fitness\n",
        "            for i in range(_lambda):\n",
        "                    new_particle = mutate(particle) #first mutation\n",
        "                    temp = find_fitness(new_particle)\n",
        "                    if temp < target_fitness:\n",
        "                        particle = new_particle.copy() #updation\n",
        "                        target_fitness = temp\n",
        "                    elif (temp<=(1+delta)*target_fitness):\n",
        "                        temp_particle = new_particle.copy()\n",
        "                        for j in range(_lambda):\n",
        "                            temp_particle1 = mutate(temp_particle) #second mutation\n",
        "                            temp_fitness = find_fitness(temp_particle1)\n",
        "                            if temp_fitness < target_fitness:\n",
        "                                target_fitness=temp_fitness\n",
        "                                particle=temp_particle1.copy() #updation\n",
        "                            break\n",
        "            return particle   \n",
        "\n",
        "def randomwalk(agent):\n",
        "    percent = 30\n",
        "    percent /= 100\n",
        "    neighbor = agent.copy()\n",
        "    size = np.shape(agent)[0]\n",
        "    upper = int(percent*size)\n",
        "    if upper <= 1:\n",
        "        upper = size\n",
        "    x = random.randint(1,upper)\n",
        "    pos = random.sample(range(0,size - 1),x)\n",
        "    for i in pos:\n",
        "        neighbor[i] = 1 - neighbor[i]\n",
        "    return neighbor\n",
        "\n",
        "def adaptiveBeta(agent):\n",
        "    bmin = 0.1 #parameter: (can be made 0.01)\n",
        "    bmax = 1\n",
        "    maxIter = 10 # parameter: (can be increased )\n",
        "    \n",
        "    agentFit = find_fitness(agent)\n",
        "    for curr in range(maxIter):\n",
        "        neighbor = agent.copy()\n",
        "        size = np.shape(neighbor)[0]\n",
        "        neighbor = randomwalk(neighbor)\n",
        "\n",
        "        beta = bmin + (curr / maxIter)*(bmax - bmin)\n",
        "        for i in range(size):\n",
        "            random.seed( time.time() + i )\n",
        "            if random.random() <= beta:\n",
        "                neighbor[i] = agent[i]\n",
        "        neighFit = find_fitness(neighbor)\n",
        "        if neighFit <= agentFit:\n",
        "            agent = neighbor.copy()\n",
        "            \n",
        "\n",
        "\n",
        "    return agent\n",
        "\n",
        "def find_fitness(particle):\n",
        "            features = []\n",
        "            for x in range(len(particle)):\n",
        "                    if particle[x]>=0.5: #convert it to zeros and ones\n",
        "                        features.append(df.columns[x])\n",
        "            if(len(features)==0):\n",
        "                        return 10000\n",
        "            new_x_train = train_x[features].copy()\n",
        "            new_x_test = test_x[features].copy()\n",
        "\n",
        "            _classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "            _classifier.fit(new_x_train, train_y)\n",
        "            predictions = _classifier.predict(new_x_test)\n",
        "            acc = accuracy_score(y_true = test_y, y_pred = predictions)\n",
        "            fitness = acc\n",
        "            err=1-acc\n",
        "            num_features = len(features)\n",
        "            fitness =  alpha*err + (1-alpha)*(num_features/total_features)\n",
        "\n",
        "            return fitness\n",
        "\n",
        "def transfer_func(velocity): #to convert into an array of zeros and ones\n",
        "            t=[]\n",
        "            for i in range(len(velocity)):\n",
        "                    t.append(abs(velocity[i]/(math.sqrt(1+velocity[i]*velocity[i])))) #transfer function inside paranthesis\n",
        "            return t\n",
        "\n",
        "#initialize swarm position and swarm velocity of SSD\n",
        "swarm_vel = np.random.uniform(low=0, high=1, size=(swarm_size,total_features))\n",
        "\n",
        "swarm_pos = np.random.uniform(size=(swarm_size,total_features))\n",
        "swarm_pos = np.where(swarm_pos>=0.5,1,0)\n",
        "\n",
        "c = 100\n",
        "alpha= 0.9\n",
        "\n",
        "gbest_fitness=100000\n",
        "pbest_fitness = np.zeros(swarm_size)\n",
        "pbest_fitness.fill(np.inf)  #initialize with the worse possible values\n",
        "pbest = np.empty((swarm_size,total_features))\n",
        "gbest = np.empty(total_features)\n",
        "pbest.fill(np.inf)\n",
        "gbest.fill(np.inf)\n",
        "\n",
        "for itr in range(max_iterations):\n",
        "\n",
        "                for i in range(swarm_size):\n",
        "                  \n",
        "                    swarm_pos[i] = adaptiveBeta(swarm_pos[i]) #for ABHC local search\n",
        "                    #swarm_pos[i] = LAHC(swarm_pos[i]) #for LAHC local search\n",
        "                    fitness = find_fitness(swarm_pos[i])\n",
        "\n",
        "                    if fitness < gbest_fitness:\n",
        "\n",
        "                        gbest=swarm_pos[i].copy() #updating global best\n",
        "                        gbest_fitness=fitness\n",
        "\n",
        "\n",
        "\n",
        "                    if fitness < pbest_fitness[i]:\n",
        "                        pbest[i] = swarm_pos[i].copy() #updating personal best\n",
        "                        pbest_fitness[i]=fitness\n",
        "\n",
        "                    r1 = random.random()\n",
        "                    r2 = random.random()\n",
        "\n",
        "          #updating the swarm velocity\n",
        "                    if r1 < 0.5:\n",
        "                        swarm_vel[i] = c*math.sin(r2)*(pbest[i]-swarm_pos[i]) +math.sin(r2)* (gbest-swarm_pos[i])\n",
        "                    else:\n",
        "                        swarm_vel[i] = c*math.cos(r2)*(pbest[i]-swarm_pos[i]) + math.cos(r2)*(gbest-swarm_pos[i])\n",
        "                    \n",
        "          #decaying value of c\n",
        "                    alpha= 0.9\n",
        "                    c=alpha*c;\n",
        "          \n",
        "          #applying transfer function and then updating the swarm position\n",
        "                    t = transfer_func(swarm_vel[i])\n",
        "                    for j in range(len(swarm_pos[i])):\n",
        "                        if(t[j] < 0.5):\n",
        "                            swarm_pos[i][j] = swarm_pos[i][j]\n",
        "                        else:\n",
        "                            swarm_pos[i][j] = 1 - swarm_pos[i][j]\n",
        "\n",
        "\n",
        "\n",
        "selected_features = gbest\n",
        "print(gbest_fitness)\n",
        "            \n",
        "number_of_selected_features = np.sum(selected_features)\n",
        "print(\"#\",number_of_selected_features)\n",
        "\n",
        "features=[]\n",
        "for j in range(len(selected_features)):\n",
        "                if selected_features[j]==1:\n",
        "                        features.append(df.columns[j])\n",
        "new_x_train = train_x[features]\n",
        "new_x_test = test_x[features]\n",
        "\n",
        "_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "_classifier.fit(new_x_train, train_y)\n",
        "predictions = _classifier.predict(new_x_test)\n",
        "acc = accuracy_score(y_true = test_y, y_pred = predictions)\n",
        "pre = precision_score(y_true = test_y, y_pred = predictions,average=None)\n",
        "rec = recall_score(y_true = test_y, y_pred = predictions, average=None)\n",
        "result = classification_report(y_true = test_y, y_pred = predictions, digits=5)\n",
        "fitness = acc\n",
        "print(\"Acc:\",fitness)\n",
        "print(\"Precision:\", pre)\n",
        "print(\"Recall:\",rec)\n",
        "print(result)\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "IljQ9GU_pT-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To plot the ROC curve**"
      ],
      "metadata": {
        "id": "jMO2EpTkfEe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "fpr, tpr, thresholds = roc_curve(test_y, predictions)\n",
        "\n",
        "auc = auc(fpr, tpr)\n",
        "sc = roc_auc_score(test_y, predictions)\n",
        "print(auc)\n",
        "print(sc)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1],\"--\")\n",
        "\n",
        "plt.plot(fpr2, tpr2, label='ROC curve (area = {:.3f})'.format(auc), color='orange')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZH18BcYFp6ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm: Local Search based GSA, WOA, GWO, GA, PSO, SCA, HS, & EO.**"
      ],
      "metadata": {
        "id": "r5YOnDYxdbE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn\n",
        "!pip install ReliefF\n",
        "!pip install Py-FS==0.0.39"
      ],
      "metadata": {
        "id": "hQRlIdMOqRnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.LS_GA import GA\n",
        "from utils.LS_GSA import GSA\n",
        "from utils.LS_GWO import GWO\n",
        "from utils.LS_WOA import WOA\n",
        "from utils.LS_PSO import PSO\n",
        "from utils.LS_SCA import SCA\n",
        "from utils.LS_EO import EO\n",
        "from utils.LS_HS import HS"
      ],
      "metadata": {
        "id": "fDxjjT0WrPku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#solution, result = GA(20, 100, x, y, save_conv_graph=True)   # To run Local search based GA algorithm\n",
        "#solution, result = GSA(20, 100, x, y, save_conv_graph=True)  # To run Local search based GSA algorithm\n",
        "#solution, result = GWO(20, 100, x, y, save_conv_graph=True)  # To run Local search based GWO algorithm\n",
        "solution, result = WOA(20, 100, x, y, save_conv_graph=True)   # To run Local search based WOA algorithm\n",
        "#solution, result = PSO(20, 100, x, y, save_conv_graph=True)  # To run Local search based PSO algorithm\n",
        "#solution, result = SCA(20, 100, x, y, save_conv_graph=True)  # To run Local search based SCA algorithm\n",
        "#solution, result = EO(20, 100, x, y, save_conv_graph=True)   # To run Local search based EO algorithm\n",
        "#solution, result = HS(20, 100, x, y, save_conv_graph=True)   # To run Local search based HS algorithm\n",
        "print(result.accuracy)\n",
        "print(result.precision)\n",
        "print(result.recall)"
      ],
      "metadata": {
        "id": "m8c35Q62eTTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAiy4nwDcc5bEkAR8oXj3T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}